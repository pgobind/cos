import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;

import java.util.*;
import java.util.stream.Collectors;
import java.util.stream.IntStream;

public class SparkSubsetSolver {

    private static List<Integer> getRowSums(List<Row> rows) {
        return rows.stream()
                .map(row -> row.getAs("col1"))
                .collect(Collectors.toList());
    }

    private static List<Integer> getSubsetSum(List<Row> rows, int target) {
        List<Integer> sumsForRows = getRowSums(rows);

        List<Integer>[] dp = new ArrayList[target + 1];
        dp[0] = new ArrayList<>();

        IntStream.range(0, sumsForRows.size()).forEach(i -> {
            int num = sumsForRows.get(i);
            IntStream.rangeClosed(num, target).map(j -> target + num - j).forEachOrdered(j -> {
                if (dp[j - num] != null) {
                    List<Integer> subset = new ArrayList<>(dp[j - num]);
                    subset.add(i);
                    dp[j] = subset;
                }
            });
        });

        return IntStream.rangeClosed(0, target)
                .map(i -> target - i)
                .filter(i -> dp[i] != null)
                .mapToObj(i -> dp[i])
                .findFirst()
                .orElse(null);
    }

    public static List<Row> closestSubsetSum(List<Row> rows, int target) {
        List<Integer> subsetIndices = getSubsetSum(rows, target);
        return subsetIndices == null ? null : subsetIndices.stream().map(rows::get).collect(Collectors.toList());
    }

    public static Map<String, List<Row>> processDatasets(Dataset<Row> ds1, Dataset<Row> ds2) {
        Map<String, List<Row>> result = new HashMap<>();

        // For each 'a' value in ds2
        ds2.collectAsList().forEach(rowDs2 -> {
            int targetValue = rowDs2.getAs("sum");
            String aValue = rowDs2.getAs("a");

            // Filter rows in ds1 where 'a' matches aValue
            List<Row> ds1RowsForA = ds1.filter(row -> row.getAs("a").equals(aValue)).collectAsList();

            List<Row> closestSubset = closestSubsetSum(ds1RowsForA, targetValue);

            if (closestSubset != null) {
                int closestSubsetSum = closestSubset.stream().mapToInt(row -> row.getAs("col1")).sum();
                if (closestSubsetSum == targetValue) {
                    result.put("Exact Match", closestSubset);
                } else {
                    result.put("Closest Match", closestSubset);
                }
            } else {
                result.put("No Closest Match", ds1RowsForA);
            }
        });

        return result;
    }
}



import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.Encoders;
import static org.apache.spark.sql.functions.*;

public class SparkSubsetExample {
    public static void main(String[] args) {
        SparkSession spark = SparkSession.builder()
                .appName("SubsetExample")
                .master("local[1]")
                .getOrCreate();

        // Sample Datasets
        List<Row> data1 = Arrays.asList(
                RowFactory.create("A", 10),
                RowFactory.create("A", 15),
                RowFactory.create("A", 5),
                RowFactory.create("B", 20),
                RowFactory.create("B", 5)
        );

        List<Row> data2 = Arrays.asList(
                RowFactory.create("A", 30),  // Exact match for A in ds1: 10 + 15 + 5 = 30
                RowFactory.create("B", 15)   // Closest match for B in ds1: 5 + 10 (not present, but assume closest)
        );

        Dataset<Row> ds1 = spark.createDataFrame(data1, new StructType().add("a", "string").add("col1", "integer"));
        Dataset<Row> ds2 = spark.createDataFrame(data2, new StructType().add("a", "string").add("sum", "integer"));

        // ... (your matching logic)

        spark.stop();
    }
}


import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import static org.apache.spark.sql.functions.*;

public class SparkSubsetExample {

    private static SparkSession spark;

    public static Dataset<Row> getExactMatch(Dataset<Row> ds1, String aVal, int target) {
        List<Row> ds1RowsForA = ds1.filter(col("a").equalTo(aVal)).collectAsList();
        List<Row> exactMatchRows = equalSubsetSum(ds1RowsForA, target);
        
        if (exactMatchRows != null && !exactMatchRows.isEmpty()) {
            return spark.createDataFrame(exactMatchRows, ds1.schema());
        }
        
        return null;
    }

    public static void main(String[] args) {
        spark = SparkSession.builder()
                .appName("SubsetExample")
                .master("local[1]")
                .getOrCreate();

        // Sample Datasets ... (same as before)

        // For each unique value in ds2, we check for an exact match
        Dataset<Row> ds2UniqueVals = ds2.select("a").distinct();
        List<Row> uniqueValsList = ds2UniqueVals.collectAsList();

        for (Row uniqueRow : uniqueValsList) {
            String aVal = uniqueRow.getAs("a");
            int targetSum = ds2.filter(col("a").equalTo(aVal)).first().<Integer>getAs("sum");
            
            Dataset<Row> exactMatchResult = getExactMatch(ds1, aVal, targetSum);
            
            if (exactMatchResult != null) {
                exactMatchResult.show();
            }
        }
        
        spark.stop();
    }
}


