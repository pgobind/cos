import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;

public class SparkSQLJoinAggregation {

    public static void main(String[] args) {
        SparkSession spark = SparkSession.builder()
                .appName("Spark SQL Join Aggregation")
                .getOrCreate();

        // Assuming the following DataFrames are already available
        // Dataset<Row> meritBalances = ...
        // Dataset<Row> orgOuXref = ...
        // Dataset<Row> scaGlXref = ...

        // Register the DataFrames as temporary views
        meritBalances.createOrReplaceTempView("merit_balances");
        orgOuXref.createOrReplaceTempView("org_ou_xref");
        scaGlXref.createOrReplaceTempView("sca_gl_xref");

        // Perform the join and aggregation using Spark SQL
        Dataset<Row> result = spark.sql(
                "INSERT INTO payables_balances " +
                "SELECT " +
                "   CAST(m.id_le AS INT), " +
                "   CAST(m.id_ou AS INT), " +
                "   m.id_cy_iso, " +
                "   CAST(CAST(s.id_gl_acct AS NUMERIC) AS STRING) as id_gl_acct, " +
                "   SUM(CAST(m.am_balance AS DECIMAL(38, 20))) as tot_balance " +
                "FROM merit_balances m " +
                "INNER JOIN org_ou_xref o ON m.id_ou = o.x_id_ou " +
                "INNER JOIN sca_gl_xref s ON m.id_gl_xref = s.x_sca_gl_xref " +
                "WHERE CAST(m.dt_cob AS DATE) = 'Dategemi' " +
                "   AND m.pn_ulq = 'n' " +
                "   AND o.id_merit_le = s.id_corp_entity " +
                "   AND o.id_le = s.id_le " +
                "GROUP BY m.id_le, m.id_ou, m.id_cy_iso, s.id_gl_acct"
        );

        // If you need to store the result, you can use the following line
        // result.write().mode(SaveMode.Overwrite).saveAsTable("payables_balances");

        spark.stop();
    }
}
