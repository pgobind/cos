from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder.appName("Conditional Update in Spark SQL").getOrCreate()

# Assuming the DataFrames have already been created and are named as follows:
# rec_pl_3way_df and prtf_gmi_ac_df
# Also assuming that you have registered these DataFrames as temporary views
rec_pl_3way_df.createOrReplaceTempView("rec_pl_3way")
prtf_gmi_ac_df.createOrReplaceTempView("prtf_gmi_ac")

# Replace @recdate with the actual value you want to use
recdate = "2024-02-28"  # Example date, use the actual date you need

# Write the Spark SQL equivalent of the update statement
spark.sql(f"""
    SELECT
        t1.*,
        COALESCE(t2.id_gmi_cust, t1.id_gmi_cust) as id_gmi_cust,
        COALESCE(t2.id_prtf_fo, t1.rms_book) as rms_book,
        CASE
            WHEN t2.id_gmi_cust IS NOT NULL AND t1.id_system = t2.id_system AND t1.id_office = t2.id_ofc AND t1.id_group = t2.id_group
            THEN t1.control + 10
            ELSE t1.control
        END as control
    FROM
        rec_pl_3way t1
    LEFT JOIN
        prtf_gmi_ac t2
    ON
        t1.dt_load_cob = '{recdate}'
        AND t1.id_system = t2.id_system
        AND t1.id_office = t2.id_ofc
        AND t1.id_group = t2.id_group
""").createOrReplaceTempView("updated_rec_pl_3way")

# The 'updated_rec_pl_3way' view will have the 'id_gmi_cust', 'rms_book', and 'control' columns with updated values where conditions are met.
