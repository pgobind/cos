<dependency>
  <groupId>com.amazonaws</groupId>
  <artifactId>aws-java-sdk</artifactId>
  <version>${aws-java-sdk.version}</version>
</dependency>
<dependency>
  <groupId>com.amazonaws</groupId>
  <artifactId>aws-java-sdk-glue</artifactId>
  <version>${aws-java-sdk.version}</version>
</dependency>
<dependency>
  <groupId>com.amazonaws</groupId>
  <artifactId>aws-java-sdk-s3</artifactId>
  <version>${aws-java-sdk.version}</version>
</dependency>
<dependency>
  <groupId>org.apache.hadoop</groupId>
  <artifactId>hadoop-aws</artifactId>
  <version>${hadoop.version}</version>
</dependency>
<dependency>
  <groupId>org.apache.spark</groupId>
  <artifactId>spark-core_2.11</artifactId>
  <version>${spark.version}</version>
  <scope>provided</scope>
</dependency>
<dependency>
  <groupId>org.apache.spark</groupId>
  <artifactId>spark-sql_2.11</artifactId>
  <version>${spark.version}</version>
  <scope>provided</scope>
</dependency>
<dependency>
  <groupId>com.databricks</groupId>
  <artifactId>spark-avro_2.11</artifactId>
  <version>${spark-avro.version}</version>
  <scope>provided</scope>
</dependency>




<parent>
  <groupId>com.example</groupId>
  <artifactId>my-glue-job</artifactId>
  <version>1.0.0-SNAPSHOT</version>
</parent>
<dependencies>
  <dependency>
    <groupId>com.amazonaws</groupId>
    <artifactId>aws-java-sdk</artifactId>
  </dependency>
  <dependency>
    <groupId>com.amazonaws</groupId>
    <artifactId>aws-java-sdk-glue</artifactId>
  </dependency>
</dependencies>

package com.example.mygluejob.etl;

import com.amazonaws.services.glue.GlueContext;
import com.amazonaws.services.glue.util.Job;
import org.apache.spark.sql.SparkSession;

public class EtlJob {

  public static void main(String[] args) {
    // Create a new GlueContext and SparkSession
    GlueContext glueContext = new GlueContext(new SparkContext(new SparkConf()));
    SparkSession spark = glueContext.getSparkSession();

    // Define the input and output paths
    String inputPath = "s3://my-bucket/input";
    String outputPath = "s3://my-bucket/output";

    // Define the job script and run it
    Job job = Job

